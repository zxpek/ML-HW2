{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Assignment 2\n",
    "\n",
    "# Group 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"SMSSpamCollection\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-process messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noPunctuation(s):\n",
    "    return ''.join(ch for ch in s if ch.lower() in '''abcdefghijklmnopqrstuvwxyz ''')\n",
    "    \n",
    "df[1] = [noPunctuation(s).lower() for s in df[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Shuffle and split messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "sampling = np.random.permutation(df.shape[0])\n",
    "\n",
    "train = df.iloc[sampling[:2500]]\n",
    "validation = df.iloc[sampling[2500:3500]]\n",
    "test = df.iloc[sampling[3500:]]\n",
    "\n",
    "trainHam = train.ix[train[0]=='ham'][1].tolist()\n",
    "trainSpam = train.ix[train[0]=='spam'][1].tolist()\n",
    "\n",
    "tv = df.iloc[sampling[:3500]]\n",
    "tvHam = tv.ix[tv[0]=='ham'][1].tolist()\n",
    "tvSpam = tv.ix[tv[0]=='spam'][1].tolist()\n",
    "\n",
    "validationText = validation[1]\n",
    "validationLabels = validation[0]\n",
    "\n",
    "testText = test[1]\n",
    "testLabels = test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesForSpam:\n",
    "    def train (self, hamMessages, spamMessages):\n",
    "        self.words = set(\" \".join (hamMessages + spamMessages).split()) #Create a set of all words used in all texts\n",
    "        self.priors = np.zeros(2)\n",
    "        self.priors[0] = float(len(hamMessages)) / (len(hamMessages) + len(spamMessages)) #Probability of ham\n",
    "        self.priors[1] = 1.0 - self.priors[0] #Probability of spam\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m])) / len(hamMessages) #P(W|Ham)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m])) / len(spamMessages) #P(W|Spam)\n",
    "            self.likelihoods.append([min(prob1,0.95),min(prob2,0.95)]) #Prevents probabilities from going too close to 1 for div0 errors\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "\n",
    "    def train2 (self, hamMessages, spamMessages):\n",
    "        self.words = set(\" \".join (hamMessages + spamMessages).split()) #Create a set of all words used in all texts\n",
    "        self.priors = np.zeros(2)\n",
    "        self.priors[0] = float(len(hamMessages)) / (len(hamMessages) + len(spamMessages)) #Probability of ham\n",
    "        self.priors[1] = 1 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m])) / len(hamMessages) #P(W|Ham)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m])) / len(spamMessages) #P(W|Spam)\n",
    "            if prob1 * 20 < prob2: #Only store words where word is 20 times more likely to appear in spam than in ham\n",
    "                self.likelihoods.append([min(prob1,0.95),min(prob2,0.95)])\n",
    "                spamkeywords.append(w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "\n",
    "    def predict (self, message):\n",
    "        posteriors = np.copy (self.priors)\n",
    "        for i, w in enumerate (self.words):\n",
    "            if w in message.lower():\n",
    "                posteriors *= self.likelihoods[:,i]\n",
    "            else:\n",
    "                posteriors *= np.ones(2) - self.likelihoods[:,i]\n",
    "            posteriors = posteriors / np.linalg.norm(posteriors) #Normalising\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]\n",
    "\n",
    "    def score (self,messages,labels):\n",
    "        confusion = np.zeros(4).reshape(2,2)\n",
    "        for m, l in zip(messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float (confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explaining the code\n",
    "\n",
    "$train1$ and $train2$ starts with very similar code. They first create a set of all words used in the spam and ham messages. Next, they calculate the prior probabilities $\\mathbb{P}(Spam)$ and $\\mathbb{P}(Ham)$. From here, the two algorithms differ.\n",
    "\n",
    "$train1$ calculates $\\mathbb{P}(W|Spam)$ and $\\mathbb{P}(W|Ham)$ for all words and saves the likelihoods. It also sets a maximum on the likelihood of any word at 0.95.\n",
    "\n",
    "$train2$ also calculates $\\mathbb{P}(W|Spam)$ and $\\mathbb{P}(W|Ham)$. However, it only saves the likelihoods of words where $\\mathbb{P}(W|Spam) > 20 \\cdot \\mathbb{P}(W|Ham)$, where the liklihood of a word appearing in a Spam message is 20 times higher than the likelihood of the word appearing in a Ham message.\n",
    "\n",
    "The $predict$ function gives the predicted label based on the posterior probability of a message being Spam or Ham. The Naive Bayes assumption is used in this step. For each word in the set of words created in the training step, if the word appears in the message, we multiply the posterior probability by the likelihood of that word in Spam and Ham. Otherwise, we multiply the posterior by (1 - likelihood) of the word in Spam and Ham. We are able to simply multiply the likelihood based on the Naive Bayes assumption of conditional independence. Finally, the posterior probability is normalised so that $\\mathbb{P}(Spam|X) + \\mathbb{P}(Ham|X) = 1$. Then the algorithm returns the label for the higher probability.\n",
    "\n",
    "Finally, the $score$ function goes through every message given and calls the $predict$ function on it. It then checks the predicted label with the actual label and increases the count in the confusion matrix accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train classifers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayesForSpam()\n",
    "nb.train(trainHam,trainSpam)\n",
    "\n",
    "nb2 = NaiveBayesForSpam()\n",
    "nb2.train2(trainHam,trainSpam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Performance of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = [nb.predict(text)[0] for text in validationText]\n",
    "errors = np.mean(labels != validationLabels)\n",
    "\n",
    "labels2 = [nb2.predict(text)[0] for text in validationText]\n",
    "errors2 = np.mean(labels2 != validationLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from train1 is 0.041\n",
      "Error from train2 is 0.03\n"
     ]
    }
   ],
   "source": [
    "print('Error from train1 is', errors)\n",
    "print('Error from train2 is', errors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the error from train2 is significantly lower than the error from train1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Speed and accuracy of classifiers\n",
    "\n",
    "We expect $train2$ to be much faster than $train1$ simply because the length of saved words in $train2$ is much shorter than $train1$, since we only saved words that have a significantly higher chance of being a spam key word.\n",
    "\n",
    "The increased accuracy is expected as well. Consider common words such as \"and\" or \"the\". These words could be very prevalent in Spam messages and would increase the posterior probability of a message being Spam. However, these words are also commonly used in Ham messages. Thus, by only considering words that occur much more often in Spam, we reduce the chance of having false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 false positives using train1\n",
      "There are 1 false positives using train2\n"
     ]
    }
   ],
   "source": [
    "falsepos1 = sum([a and b for a, b in zip(pd.Series(labels) == 'spam', validationLabels == 'ham')])\n",
    "falsepos2 = sum([a and b for a, b in zip(pd.Series(labels2) == 'spam', validationLabels == 'ham')])\n",
    "\n",
    "print('There are',falsepos1, 'false positives using train1')\n",
    "print('There are',falsepos2, 'false positives using train2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more moderate approah would be to increase the threshold to classify a message as spam. In the above algorithm, we classified a message as Spam if $\\mathbb{P}(Spam) > 0.5$. We could instead increase the threshold to a higher value such as $\\mathbb{P}(Spam) > 0.7$. \n",
    "\n",
    "We could also increase the threshold to 1, where we classify ALL messages as Ham. By simply classifying all messages as Ham, we reduce the number of false positives to 0, but at the expense of large number of false negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Confusion matrix of train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb3 = NaiveBayesForSpam()\n",
    "nb3.train2(tvHam,tvSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testError, confusion = nb3.score(testText,testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97249034749\n",
      "[[ 1800.    53.]\n",
      " [    4.   215.]]\n"
     ]
    }
   ],
   "source": [
    "print(testError)\n",
    "print(confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
